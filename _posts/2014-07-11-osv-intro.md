---
layout: page
title: "OSv 介绍"
description: ""
category:
tags: []
---
{% include JB/setup %}
# OSv介绍
##设计目标：
* 运行现有的云应用（linux 兼容）
* 这些应用运行的比linux快
* 尽量减少映像大小，启动快，有配置更改就启动一个新的vm
* 为应用程序提供新的API,能提高性能
* 在通用运行环境中使用这些API,比如JVM. 
* 作为一个guest OS的研究平台。采用C++11实现，代码较少

# 设计和实现
	osv遵循library OS设计,hypervisor作为exokernel,VM最为应用程序：每一个VM是一个应用程序，并拥有一个libraryOS。LibraryOS的设计目标是解决应用程序在性能和功能方面的限制，这些限制多源自传统OS的设计。它把资源管理放到应用层，通过安全的API直接把硬件暴露给应用程序，减少抽象和保护的层数。
	OSv在每一个VM中运行一个应用程序。如果要运行多个互不信任的应用程序，他们可以运行在不同的VM中。更为重要的是，减少了guest中多余和有代价的“隔离”。因此,OSv使用单一地址空间，所有的线程和内核使用一个page tables.这减少了上下文切换，包括应用线程间的切换以及应用线程和内核间的切换。
	OSv内核使用一个ELF动态linker加载要运行的应用程序。 这个linker接受标准的ELF文件，在linux下编译的均可。当这些代码调用Linux API(比如glibc某个函数），linker会处理这些调用，交由OSv内核中的实现。linux上的“系统调用”比如read()等在OSv中都是普通的函数调用，不会产生额外的负载，不用进行user-to-kernel间的参数拷贝。
	OSv的核心代码是全新的，用C++11实现的。包括loader, 动态linker, 内存管理，线程调度， 同步机制比如mutex和RCU, 虚拟硬件驱动等。
	OSv中的硬件驱动实现更为简单。包括一部分传统的由hypervisor模拟的PC设备,比如键盘，VGA, 串口， SATA, IDE 和HPET. 它还包括许多paravirtual 驱动， 包括clock, NIC, 和block。
	OSv的文件系统符合UNIX-like VFS接口，并采用ZFS作为默认的文件系统。也支持其他的文件系统，比如ramfs, devfs, procfs.
	OSv的网络实现从FreeBSD中借鉴，并对它进行了重构，采用了基于network channel的设计。
	
## 内存管理
    理论上，libraryOS可以采用扁平的物理内存映射，但是OSv采用了虚拟内存，类似通用的OS。这里有两个原因，第一x86_64架构在long模式中使用虚拟内存；第二，应用程序会使用map和unmap 内存，并使用page保护机制。
    OSv通过mmap API支持demand paging和内存映射。这对于哪些基于jvm的，并调用mmap的应用非常关键，比如NoSQL数据库Cassandra.
    OSv采用huge page(2MB). 使用huge page能减少应用程序的TLB miss,从而提高性能。
    因为mapping可以被部分unmapped, 所以一些page需要被分散为一些小page. OSv通过使用类似linux Transparent Huge page的机制处理。
    
## 没有Spinlock
    在SMP机器上需要使用spin-lock. 在单核系统上，比较容易饱和一个数据结构被并发访问，比如禁止中断。但是在多核系统上，这是不行的。SMP使用spin-lock解决该问题：一个CPU使用一个test-and-set 操作获取锁，其它的CPU执行死循环直到能获取这些锁。SMP OS使用spin-lock实现高层次的lock机制，比如sleeping mutex, 并在无法sleep的上下文中直接使用spin-lock.比如scheduler自身和中断处理上下文。
    在SMP硬件环境上spin-lock还是很适用的。但是在虚拟环境下，这会导致“lock-holder preemption"问题：如果OS想让物理CPU运行，他们就可以运行，但是虚拟CPU会”pause"未知的时间。vm-exit或者hypervisor决定运行另一个guest, 或者hypervisor要运行在该CPU上。
    如果一个虚拟CPU在获取spin-lock的时候被暂停了，其它想要获取该lock的CPU只能无望的在浪费CPU资源。当一个mutex的实现是基于spin-lock, 这意味着一个线程在等待一个lock,会发现自己在spinning，在浪费CPU时间，而不是立即sleep并让另一个线程运行。“lock-holder preemption"问题会导致性能低下，有研究表明，在同一个CPU上运行两个VM会使性能下降7%到99%在极端情况下。
    为了减轻该问题，已经提出了很多办法，通常是修改hypervisor或hypervisor和guest的交互。但是，在一个为虚拟化设计的内核中，更好的办法是完全避免该问题。OSv完全没有使用spin-lock.
    消除spin-lock的一个方法是使用lock-free算法。这些算法聪明的使用SMP中的一些原子操作（比如compare-exchange, fetch-and-add)来保证并发环境下得数据结构的一致性。我们还可以使用RCU技术。在OSv中我们采用如下方法：
    1.保证内核中的大部分工作包括中断处理都是在线程中处理；
    2.实现mutex不采用spinlock, 使用lock-free算法
    3.scheduler自身不能在一个线程中，所以采用没有spin-lock的实现保护其数据，使用了每个CPU的run queue和lock-free算法。
    OSv尽量在普通线程中运行所有任务。中断处理会叫醒一个线程处理该中断。内核代码和应用程序一样运行在线程中，可以sleep, preempted. OSv采用轻量级的线程上下文切换。
    我们的mutex实现基于Gidenstam & Papatriantafilou的lock-free算法，其内部数据结构采用原子化操作。采用我们的lock-free mutex, 一个暂停的CPU不可能导致其他CPU开始spining.从而避免“lock-holder preemption"问题。
    最后，scheduler使用per-CPU的运行队列，所以大部分的调度决定都是本地CPU相关的，不需要锁。当需要协同多个CPU时，详见线程调度章节。
## 网络Channels
    云中的操作系统必须提供高质量的TCP/IP协议栈。OSv采用Van Jacobson的"net channel"设计思路。
    典型的网络协议栈包括两个方向的流量：
    Top-down: send() 和recv() 系统调用在socket层开始，把用户层缓存转换为TCP包，附加上IP头，最终通过网卡驱动出去。
    Bottom-up: 网卡驱动接收到进来的报文，通过IP层解析，接着是TCP层，然后放到socket缓存中，唤醒必要的被阻塞的send(), recv()和poll()系统调用。
    在传统的网络协议栈中，中断上下文和应用线程上下文都在网络协议栈上处理，他们会访问共享的数据，这就需要用到锁。
    OSv中几乎所有的packet处理由应用线程处理。当接收到packet, 通过一个简单的分发，转给一个channel, 这个channel是一个单生产者/单消费者队列，它会把报文转给应用线程。每一个channel对应一个流，比如一个TCP连接或者一个UDP路径。这样多个线程就不会并发访问共享数据。使用net channel方法可以大幅较少锁的需求。
    
## 线程调度
    线程调度将从以下几个方面讨论，lock-free, preemptive, tick-less, 公平，扩张性，效率。
    - Lock-free
    OSv调度器不应该使用spin-lock, 不能使用sleeping mutex. 调度器在每一个CPU上保留一个运行队列，存储该CPU所有可运行的线程。休眠线程不在这个队列上。当运行线程要求重新调度（reschedule)或者是超时导致的强占，调度器都会开始运行。此时，调度器会从其运行队列中选择一个最合适的线程去运行，参考公平标准。因为每个CPU都有自己独立的运行队列，所以这部分的调度不需要锁。
    分离的运行队列可能会导致这样的情况，有一个CPU的队列大小超过其他CPU的队列，从而破坏了调度器的公平性。我们通过在每一个CPU中运行一个负载均衡线程。这些线程每隔一段时间运行一次，并检查其他CPU的运行队列的大小是否小于自己的队列。如果是这样，则从自己的队列中选择一个线程，并把该线程放到远程CPU的运行队列中。
    在远程CPU上激活一个线程需要更为精细的lock-free算法：在N个CPU中的每一个，设置N个无锁队列，处理激活请求，总过有N*N个队列。同时每个CPU保留一个非空队列的bitmask. 当CPU s 想在CPU d上激活一个线程，它把该线程加入到（s,d)队列中，并置CPU d的bitmask的s位，然后向CPU d发送IPI(inter-processor interrupt).该中断会导致CPU d执行一次调度，通过查找bitmask找到对应的队列。
    - Preemptive
    OSv支持多任务强占：虽然线程会请求重新调度（waiting, yielding, 或激活其它线程），它也可以随时发生，比如被时钟或IPI.应用线程和内核线程一样，都是可被抢占的。一个线程可以临时避免被抢占，通过增加每个线程的preempt-disable counter. 这个特性在一些场景中有用，比如维护CPU变量和RCU锁。当一个线程延迟被强占时，如果发生中断，它不会导致重新调度，但是当该线程恢复可抢占模式时，就会重新开始调度。
    - Tick-less
    现代内核通常使用一个周期性的时钟中断，被称为tick。该tick可以触发周期性的线程的调度，比如1秒100次。一些内核会统计各个线程运行的tick数，用于在调度中参考。tick比较浪费CPU时间，特别是在虚拟化环境中。
    OSv采用tickless的设计。使用一个高精度的clock,调度器统计每个线程的精确时间而不是运行的tick数。当公平调度器决定下一个要运行的线程时，同时计算下一次要调度的时间。避免在两个高负载的线程间来回切换。
    - 公平性
    每次调度时，调度器需要决定下一个要运行的线程，以及运行时间。OSv计算每个线程运行时间的“exponentially-decaying moving average",并选择最小的moving-average runtime线程。
    - 扩展性
    OSv调度器的时间复杂度是O(lgN): 运行队列按照moving-average时间排序，每次调度只更新一个线程的运行时间。
    - 效率
    * 单地址空间当切换上下文时不需要更换page talbe，flush TLB.
    * 当切换上下文时不保留FPU状态

# 独有的接口
## Shrinker
## JVM Balloon

# 评测
## Macro Benchmarks
## Memcached
## SPECjvm2008
## Micro Benchmarks
### Network performance

    
