---
layout: page
title: "Glusterfs AFR代码分析"
description: ""
category: 
tags: []
---
{% include JB/setup %}
#Glusterfs AFR代码分析
##原理及流程
###读操作
所有不修改文件或目录的操作会向所有子卷发送请求，其中第一个成功响应会返回给应用程序。但是，read()系统调用是一个例外。对于read()操作，afr会采用负载均衡的方式把请求发送到特定的服务器上。
####文件操作分类
afr把对文件系统的写操作分为以下三类：
 * 数据（data）：修改文件内容的操作（write,truncate）
 * 元数据（metadata）:修改文件或目录的属性（比如权限，所有者）
 * 目录项（entry）：创建或删除目录项（比如mkdir, create, rename, rmdir, unlink等）

####锁和修改日志（change log）
为了保证所有子卷的一致性，当对文件或目录要进行修改时，afr采用锁机制进行保护。默认的，afr把第一个子卷作为主锁服务器，同时，其它锁服器的个数可以增加，最多等于子卷的个数。
修改日志是afr保存的和文件或目录相关的扩展属性集合。修改日志记录的对文件或目录（data,medata,entry）的修改,所以自修复算法可以依次判断文件或目录的最近版本。
###写算法
写算法（data, medata, entry）的大致流程如下：
1.	在所有锁服务器上把文件或目录锁住
2.	向所有服务器写修改日志
3.	执行操作
4.	删除修改日志
5.	在所有锁服务器上释放锁
###自修复
如果afr检测到同一文件的多个副本间有不一致的现象，会尝试进行修复。它使用修改日志中的内容判断哪一份拷贝是正确的版本。
如果要修复的是目录，所有子卷上都会保存正确的版本内容，通过删除或创建必要的目录项。
如果要修复的是文件：如果一个文件在某些子卷上不存在，则创建；如果文件的大小，所有者，权限等有不同则进行修复；如果修改日志显示某些副本需要更新，则进行更新。
###扩展属性
AFR中扩展属性的键值为trusted.afr.*, *可以是该副本集中的brick的名字，注意该值不会是本身brick的名字。比如，有一个AFR卷包含test1-client-0和test-client-1两个子卷，在test1-client-0上的文件可能包含一个trusted.afr.test1-client-1.这样做的目的是因为AFR是用来从故障中恢复的，因此，一个操作的状态不能在发生故障的同一个点上进行恢复。
操作有三种类型：
* 数据操作：包括write, truncates等
* 元数据操作：包括chmod/chown/chgrp, xattrs
* 目录项（namespace命名空间）操作:包括create, delete, rename等。
当文件系统发生一次修改时，首先所有节点的计数都会增加。事实上，GlusterFS定义了一个额外的xattr操作(原子性增加)为了支持AFR.一旦，所有子卷的计数都增加了，真正的写操作就会发送到所有的副本子卷上。当每个节点完成其操作时，其它节点上的计数就会减一。最终，所有节点上的计数器都会变为零。如果一个节点X在中间过程崩溃了，或者没有开始，那么副本集内其它节点对X的计数会是一个非零数。这种状态很容易被检测出来（比如stat()）,根据所有节点的计数就能推断出来那个brick过期了，同时正确的自修复也可以激活。最糟糕的情况是两个bricks同时认为对方的计数非零，这时就发生了“脑裂”。

###详细的写算法
1. 在文件上加锁。可以避免同时的更新，避免写和自修复的冲突，多个客户端的冲突。
2. 在文件副本上标记“changelog”(扩展属性)，表示pending write.这并不是一个真正的log,而是一个”diry”的计数数组，在此时增加。
3. 在所有的副本上执行实际的写操作。
4. 每当一个节点完成写操作，就再其它节点上更新（减少）计数(changelog).
5. 当所有节点写操作和changelog减少完成时，释放锁。

以上是理论算法，实际中，采用了多种优化方法，比如：
* 并不立即清除X上对Y的changelog,如果还在等待Z的消息，直到可以同时清理两者。但是，这将导致X上对Y的changelog在一段时间内过期，可能导致一个不同的自修复结果。
* 如果另一个写在执行则跳过changelog增加，同样的当前者的写操作完成时，略过changelog的减少。这被称为“changelog piggbacking”,因为这样让第二个写操作骑在(‘ride along”)第一个写操作的changelog增加上。
* 当写重叠（overlap）时忽略掉unlock和之后的lock,这被称为“eager locking”,它和change piggybacking类似，除了它是对lock 操作而不是changlog操作的优化。
使用了以上优化算法，一次写操作的network round trips可以从5次减少到1次，只是对写操作。更好的情况是，如果使用write-behind,这种访问模式更容易发生。不幸的是，许多工作负载既不允许write-behind也不提供可优化的访问模式，导致这些优化方法无法生效，并且IMO认为它们是为了性能度量，也就是对benchmarks的欺骗。

###详细的自修复算法
该模块最复杂的部分是如何利用changelog进行自修复。先以下给出术语：如果X上对Y的计数不为0，那么就说X指控(accuses)Y(没有完成操作)。从而引出每个副本的特性（characters）:
* IGNORANT:该副本甚至没有changelog,比如该副本上此文件已丢失。
* INNOCENT:该副本没有指控任何其它副本（节点）。
* FOOL:该副本指控自身。它进行了changelog的增加，而没有减少，所以我们不知道写操作是否写到硬盘上了。
* WISE:副本没有指控自身，但是指控了其它副本（节点）。
自修复决策算法是非常复杂的，以下给出给出一些重点。之前说副本没有对自身的changelog,它又如何指控自身？隐含的自我指控在某些条件下会发生-具体细节仍需分析。这里的关键是把状态的汇总和状态的判定分离，从而让判定逻辑在不同的条件下按照统一的方法。一些最常见和重要的例子有：
* 所有的节点都是innocent.在该情况下不用进行修复。
* 只有一个WISE节点存在。只有该节点完成了操作，其它节点都没完成。这唯一的WISE节点就可作为自修复的源（source）,把它的数据传播其它次WISE的节点。
* 多个WISE节点存在，当时他们没有相互指控。随便挑一个作为源。
* 多个WISE节点存在，并且之间有相互指控。这时就发生了声名狼藉的脑裂，无法自动修复。可以使用quorum enforcement特性避免这种情况。
* 不符合以上情况。挑选指控其它节点最多的FOOL节点。这里的理论是指控其它最多的节点就是一直保持up的节点，而其它节点都down了，所以它拥有正确的数据。
3.3之前采用find | stat或ls –alR在故障后启动自修复。最近，GlusterFS尝试使用一个自修复进程自动完成该操作，并且尝试记录需要自修复的文件而不是全部扫描。在GlusterFS 3.3或3.4，将会有一个自动的、高效的自修复进程。
